{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c6ede70-d2f2-4e8e-b31e-8a403eab8347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from scipy.spatial import distance\n",
    "from textblob import TextBlob\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoProcessor, Wav2Vec2ForCTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17d9fa69-12fb-47e4-8ae9-032169b6833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1d1e0d-0eb6-4acb-832c-e87dc02f14d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4498"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_txt_path_lst = sorted(glob('/home/kyoungmin_temp/laboratory/kor2kor/dataset/aihub_older_jeju/test_circum_01/*.json'))\n",
    "len(test_txt_path_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06192d44-dae1-4503-8844-464459693358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 4498/4498 [00:09<00:00, 455.04it/s]\n"
     ]
    }
   ],
   "source": [
    "test_info = {'path': [], 'dialect': [], 'standard': []}\n",
    "test_info['path'] = list(map(lambda x: x.replace('test_circum_01', 'test_speech_circum_01').replace('json', 'wav'), test_txt_path_lst))\n",
    "\n",
    "for sample_path in tqdm(test_txt_path_lst):\n",
    "    with open(sample_path) as f:\n",
    "        sample_json = json.load(f)\n",
    "    \n",
    "    dialect_txt = ' '.join(list(x['dialect'] for x in sample_json['transcription']['segments']))\n",
    "    standard = ' '.join(list(x['dialect'] if x['standard'] is None else x['standard'] for x in sample_json['transcription']['segments']))\n",
    "    test_info['dialect'].append(dialect_txt)\n",
    "    test_info['standard'].append(standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c9a5552-d13e-47e8-b56f-e2f839391e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'dialect', 'standard'],\n",
       "    num_rows: 4498\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = Dataset.from_dict(test_info)\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6ee01bc-a9f2-41f9-b613-309695993e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForCTC,\n",
    "    AutoTokenizer,\n",
    "    Wav2Vec2ProcessorWithLM,\n",
    ")\n",
    "from transformers.pipelines import AutomaticSpeechRecognitionPipeline\n",
    "\n",
    "# 모델과 토크나이저, 예측을 위한 각 모듈들을 불러옵니다.\n",
    "model = AutoModelForCTC.from_pretrained(\"42MARU/ko-spelling-wav2vec2-conformer-del-1s\", cache_dir='/home/kyoungmin_temp/HF_CACHE')\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"42MARU/ko-spelling-wav2vec2-conformer-del-1s\", cache_dir='/home/kyoungmin_temp/HF_CACHE')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"42MARU/ko-spelling-wav2vec2-conformer-del-1s\", cache_dir='/home/kyoungmin_temp/HF_CACHE')\n",
    "beamsearch_decoder = build_ctcdecoder(\n",
    "    labels=list(tokenizer.encoder.keys()),\n",
    "    kenlm_model_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db7cf2ea-bc2f-445b-a55b-19b86a4ed069",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2ProcessorWithLM(\n",
    "    feature_extractor=feature_extractor, tokenizer=tokenizer, decoder=beamsearch_decoder\n",
    ")\n",
    "\n",
    "# 실제 예측을 위한 파이프라인에 정의된 모듈들을 삽입.\n",
    "asr_pipeline = AutomaticSpeechRecognitionPipeline(\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    decoder=processor.decoder,\n",
    "    device=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cf9af2c-21dd-46df-9b4d-c66c105a3fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 4498/4498 [19:48<00:00,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_txt = []\n",
    "for sample in tqdm(test_ds):\n",
    "    raw_data, _ = librosa.load(sample['path'], sr=16000)\n",
    "    kwargs = {\"decoder_kwargs\": {\"beam_width\": 100}}\n",
    "    pred = asr_pipeline(inputs=raw_data, **kwargs)[\"text\"]\n",
    "    input_txt.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df2015c5-3bc9-4de6-b0b0-70075e32988b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4498"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c82c3143-ee9a-48d9-99f7-e024402383a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "MODEL_NAME = 'KoBART_base_v2-trial2'\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "pipe = pipeline(\n",
    "    \"translation\", model=f\"{MODEL_NAME}\", max_length=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3572c399-ef39-47ef-ad16-3077862bfe6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4498"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_txt_path_lst = sorted(glob('/home/kyoungmin_temp/laboratory/kor2kor/dataset/aihub_older_jeju/test_circum_01/*.json'))\n",
    "len(test_txt_path_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22efa2c6-5b16-4e4e-80f7-9858a41ec6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_processing(result_txt):\n",
    "    empty_space = result_txt.strip(' ').replace('\\n', '').split(' ')\n",
    "    try:\n",
    "        empty_space = empty_space[:empty_space.index('')]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if(len(empty_space) >= 2):\n",
    "        while empty_space[-1] == empty_space[-2]:\n",
    "            empty_space.pop()\n",
    "            if len(empty_space) == 2:\n",
    "                break\n",
    "\n",
    "    if(len(empty_space) >= 4):\n",
    "        while empty_space[-2:] == empty_space[-4:-2]:\n",
    "            empty_space.pop()\n",
    "            empty_space.pop()\n",
    "            if len(empty_space) == 4:\n",
    "                break\n",
    "\n",
    "        if len(empty_space) == 2:\n",
    "            pass\n",
    "        else:\n",
    "            word_set1 = set(''.join(empty_space[-2:]))\n",
    "            word_set2 = set(''.join(empty_space[-4:-2]))\n",
    "            total_set = set(''.join(empty_space[-4:])) \n",
    "                \n",
    "            while (word_set1 == total_set) or (word_set2 == total_set):\n",
    "                empty_space.pop()\n",
    "                empty_space.pop()\n",
    "                \n",
    "                word_set1 = set(''.join(empty_space[-2:]))\n",
    "                word_set2 = set(''.join(empty_space[-4:-2]))\n",
    "                total_set = set(''.join(empty_space[-4:]))\n",
    "    \n",
    "                if len(empty_space) == 2:\n",
    "                    break\n",
    "\n",
    "    if(len(empty_space) >= 6):\n",
    "        while empty_space[-3:] == empty_space[-6:-3]:\n",
    "            empty_space.pop()\n",
    "            empty_space.pop()\n",
    "            empty_space.pop()\n",
    "\n",
    "        if len(empty_space) == 3:\n",
    "            pass\n",
    "        else:\n",
    "            word_set1 = set(''.join(empty_space[-3:]))\n",
    "            word_set2 = set(''.join(empty_space[-6:-3]))\n",
    "            total_set = set(''.join(empty_space[-6:]))\n",
    "            \n",
    "            while (word_set1 == total_set) or (word_set2 == total_set):\n",
    "                empty_space.pop()\n",
    "                empty_space.pop()\n",
    "                empty_space.pop()\n",
    "                \n",
    "                word_set1 = set(''.join(empty_space[-3:]))\n",
    "                word_set2 = set(''.join(empty_space[-6:-3]))\n",
    "                total_set = set(''.join(empty_space[-6:]))\n",
    "    \n",
    "                if len(empty_space) == 3:\n",
    "                    break\n",
    "            \n",
    "    return ' '.join(empty_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97dd06a6-bf65-4e48-a1af-2a34dd9ccb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "import nlptutti as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35123c-5fa1-4e31-8cf2-4c5610032246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:08,  1.04s/it]"
     ]
    }
   ],
   "source": [
    "bleu_result = {'path': [], 'bleu_score': [], 'dialect': [], 'standard': [], 'predict': [], 'cer_score': []}\n",
    "\n",
    "for sample_path, conformer_output in tqdm(zip(test_txt_path_lst, input_txt)):\n",
    "    with open(sample_path) as f:\n",
    "        sample_json = json.load(f)\n",
    "    \n",
    "    dialect_txt = conformer_output\n",
    "    ground_truth = ' '.join(list(x['dialect'] if x['standard'] is None else x['standard'] for x in sample_json['transcription']['segments']))\n",
    "    model_result = pipe(dialect_txt, num_return_sequences=1, pad_token_id=0)[0]['translation_text']\n",
    "    post_process_txt = output_processing(model_result)\n",
    "\n",
    "    reference = [ground_truth.split()]\n",
    "    model_output = post_process_txt.split()\n",
    "    bleu_score = bleu.sentence_bleu(reference, model_output)\n",
    "    cer_score = metrics.get_cer(ground_truth, model_output)['cer']\n",
    "\n",
    "    bleu_result['path'].append(os.path.basename(sample_path))\n",
    "    bleu_result['bleu_score'].append(bleu_score)\n",
    "    bleu_result['dialect'].append(dialect_txt)\n",
    "    bleu_result['standard'].append(ground_truth)\n",
    "    bleu_result['predict'].append(post_process_txt)\n",
    "    bleu_result['cer_score'].append(cer_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9dcc4c-6467-485a-9bb4-1781749029e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
