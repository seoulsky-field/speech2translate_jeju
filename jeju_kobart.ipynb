{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9519f5aa-5f63-42e5-8b2d-0eb011a0db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2, 3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2733e3fe-0df5-45b5-a31b-4eedea9190c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "918fd0b3-80e2-498e-9540-0d41f34fe14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('/home/kyoungmin_temp/laboratory/kor2kor/dataset/circum_01_train_form.csv')\n",
    "valid_csv = pd.read_csv('/home/kyoungmin_temp/laboratory/kor2kor/dataset/circum_01_valid_form.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "613b25ee-4fde-4f35-b6c6-baf08cf27831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'standard': '아빠 먼저 죽어버려서 우리 어머니가 빨리 늙었지요', 'jeju_dialect': '아방 모녀 죽어부난 우리 어멍 질레 늙어쭈마씸'}\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv['translation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26a0307c-c522-4bba-99b3-11130e55074d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation'],\n",
       "    num_rows: 29565\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = Dataset.from_pandas(train_csv)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48c31276-2d33-4807-a9d0-42bb7de3e3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation'],\n",
       "    num_rows: 6422\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds = Dataset.from_pandas(valid_csv)\n",
    "valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44981f01-0289-4e8f-93e2-7aedd973e256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 29565\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 6422\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainval_ds = DatasetDict({'train': train_ds, 'validation': valid_ds})\n",
    "trainval_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c21b7d6-1e99-4d15-8ba3-efb69c09cb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'standard': '우리 오라버니 온갖 거 다 알아 우리 오라버니에게 물어 봐', 'jeju_dialect': '우리 오라방 하간 거 다 알메 우리 오라방신디 들어 봐'}\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainval_ds[\"train\"][1][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abc18a08-0c04-4d41-880a-f35ec89d5bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'gogamza/kobart-base-v2',\n",
    "    cache_dir='/home/kyoungmin_temp/HF_CACHE'\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    'gogamza/kobart-base-v2',\n",
    "    # pad_token_id=tokenizer.eos_token_id,\n",
    "    torch_dtype='auto', low_cpu_mem_usage=True,\n",
    "    cache_dir='/home/kyoungmin_temp/HF_CACHE'\n",
    ").to(device='cuda', non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54cec129-fd26-426b-8fee-32500540ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "979a054b-d716-4a1d-b4e3-d706ac0e9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"jeju_dialect\"\n",
    "target_lang = \"standard\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [eval(example)[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [eval(example)[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=40, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88fff1ad-9195-4962-89c9-ba12bc851e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████| 29565/29565 [00:02<00:00, 14108.67 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████| 6422/6422 [00:00<00:00, 12575.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = trainval_ds.map(preprocess_function, batched=True, remove_columns=trainval_ds[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d789709-b53a-42d3-af54-d9381711b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9efb3acd-163e-4765-bbf4-75af2298123f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 29565\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6422\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9418b310-a4ce-405c-b2fb-8124ae865df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "MODEL_NAME = f\"KoBART_base_v2-trial2\"\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"./{MODEL_NAME}\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=20,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=50, \n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    run_name=MODEL_NAME,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to = [\"wandb\"]\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7e9761d-33f0-49a0-821e-2520adf90918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kyoungmin_temp/laboratory/kor2kor/wandb/run-20231219_131055-v2d42yby</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seoulsky_field/huggingface/runs/v2d42yby' target=\"_blank\">KoBART_base_v2-trial2</a></strong> to <a href='https://wandb.ai/seoulsky_field/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seoulsky_field/huggingface' target=\"_blank\">https://wandb.ai/seoulsky_field/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seoulsky_field/huggingface/runs/v2d42yby' target=\"_blank\">https://wandb.ai/seoulsky_field/huggingface/runs/v2d42yby</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2310' max='2310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2310/2310 15:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.388900</td>\n",
       "      <td>0.542521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.533900</td>\n",
       "      <td>0.432828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>0.418016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>0.416714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.406500</td>\n",
       "      <td>0.377481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.353903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.363700</td>\n",
       "      <td>0.338873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.334700</td>\n",
       "      <td>0.327471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.342800</td>\n",
       "      <td>0.308735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.287100</td>\n",
       "      <td>0.318907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>0.301617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.268500</td>\n",
       "      <td>0.295351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.260300</td>\n",
       "      <td>0.285987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.263600</td>\n",
       "      <td>0.280399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.258600</td>\n",
       "      <td>0.282129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>0.267370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.248300</td>\n",
       "      <td>0.266242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.232200</td>\n",
       "      <td>0.252505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.205200</td>\n",
       "      <td>0.263351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.183800</td>\n",
       "      <td>0.247197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.243156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>0.239162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.175600</td>\n",
       "      <td>0.231432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.169700</td>\n",
       "      <td>0.233167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.174100</td>\n",
       "      <td>0.225674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.166500</td>\n",
       "      <td>0.220381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.165500</td>\n",
       "      <td>0.209735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.153900</td>\n",
       "      <td>0.214102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.212915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.206781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>0.199940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.116100</td>\n",
       "      <td>0.199555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.118300</td>\n",
       "      <td>0.194332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.191449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.188059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.108900</td>\n",
       "      <td>0.183466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.180261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.085700</td>\n",
       "      <td>0.187285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.185736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.187053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.185192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.183430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>0.182954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.182210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>0.181994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.181988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2310, training_loss=0.2511038686289932, metrics={'train_runtime': 976.8619, 'train_samples_per_second': 151.326, 'train_steps_per_second': 2.365, 'total_flos': 2862858555648000.0, 'train_loss': 0.2511038686289932, 'epoch': 5.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04ed6236-3623-4387-a116-be3d86b6bf65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████████████████████████████████████████| 496M/496M [01:04<00:00, 7.70MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/Seoulsky/KoBART_base_v2-trial2/tree/main/'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6449db7-b754-4ef7-a6d0-830d52654cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "pipe = pipeline(\n",
    "    \"translation\", model=f\"{MODEL_NAME}\", device=device, max_length=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59a6a1f4-84ed-4cd3-8399-80219de77253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4498"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_txt_path_lst = sorted(glob('/home/kyoungmin_temp/laboratory/kor2kor/dataset/aihub_older_jeju/test_circum_01/*.json'))\n",
    "len(test_txt_path_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "f79acb78-78f6-499e-aac0-24959db0388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_processing(result_txt):\n",
    "    empty_space = result_txt.strip(' ').replace('\\n', '').split(' ')\n",
    "    try:\n",
    "        empty_space = empty_space[:empty_space.index('')]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if(len(empty_space) >= 2):\n",
    "        while empty_space[-1] == empty_space[-2]:\n",
    "            empty_space.pop()\n",
    "\n",
    "    if(len(empty_space) >= 4):\n",
    "        while empty_space[-2:] == empty_space[-4:-2]:\n",
    "            empty_space.pop()\n",
    "            empty_space.pop()\n",
    "\n",
    "        if len(empty_space) == 2:\n",
    "            pass\n",
    "        else:\n",
    "            word_set1 = set(''.join(empty_space[-2:]))\n",
    "            word_set2 = set(''.join(empty_space[-4:-2]))\n",
    "            total_set = set(''.join(empty_space[-4:])) \n",
    "                \n",
    "            while (word_set1 == total_set) or (word_set2 == total_set):\n",
    "                empty_space.pop()\n",
    "                empty_space.pop()\n",
    "                \n",
    "                word_set1 = set(''.join(empty_space[-2:]))\n",
    "                word_set2 = set(''.join(empty_space[-4:-2]))\n",
    "                total_set = set(''.join(empty_space[-4:]))\n",
    "    \n",
    "                if len(empty_space) == 2:\n",
    "                    break\n",
    "\n",
    "    if(len(empty_space) >= 6):\n",
    "        while empty_space[-3:] == empty_space[-6:-3]:\n",
    "            empty_space.pop()\n",
    "            empty_space.pop()\n",
    "            empty_space.pop()\n",
    "\n",
    "        if len(empty_space) == 3:\n",
    "            pass\n",
    "        else:\n",
    "            word_set1 = set(''.join(empty_space[-3:]))\n",
    "            word_set2 = set(''.join(empty_space[-6:-3]))\n",
    "            total_set = set(''.join(empty_space[-6:]))\n",
    "            \n",
    "            while (word_set1 == total_set) or (word_set2 == total_set):\n",
    "                empty_space.pop()\n",
    "                empty_space.pop()\n",
    "                empty_space.pop()\n",
    "                \n",
    "                word_set1 = set(''.join(empty_space[-3:]))\n",
    "                word_set2 = set(''.join(empty_space[-6:-3]))\n",
    "                total_set = set(''.join(empty_space[-6:]))\n",
    "    \n",
    "                if len(empty_space) == 3:\n",
    "                    break\n",
    "            \n",
    "    return ' '.join(empty_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "7607461d-26f7-4d35-a58d-7a78e6499dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set random index: 4217\n",
      "The random path: /home/kyoungmin_temp/laboratory/kor2kor/dataset/aihub_older_jeju/test_circum_01/st_set1_collectorjj67_speakerjj1025_55_3.json\n",
      "dialect: 무슨 걱정이시냐\n",
      "standard: 무슨 걱정이 있니\n",
      "translated: 무슨 걱정이있니 무슨 걱정이있니  걱정이있니   있니  걱정이있니                  \n",
      "post processing: 무슨 걱정이있니\n",
      "BLEU Score: 9.291879812217675e-232\n"
     ]
    }
   ],
   "source": [
    "# random_idx = random.randint(0, len(test_txt_path_lst))\n",
    "random_idx = 4217 # 744 # 542 # 3286\n",
    "# 4334\n",
    "sample_path = test_txt_path_lst[random_idx]\n",
    "print(f'Set random index: {random_idx}')\n",
    "print(f'The random path: {sample_path}')\n",
    "with open(sample_path) as f:\n",
    "    sample_json = json.load(f)\n",
    "\n",
    "dialect_txt = ' '.join(list(x['dialect'] for x in sample_json['transcription']['segments']))\n",
    "ground_truth = ' '.join(list(x['dialect'] if x['standard'] is None else x['standard'] for x in sample_json['transcription']['segments']))\n",
    "model_result = pipe(dialect_txt, num_return_sequences=1, pad_token_id=0)[0]['translation_text']\n",
    "post_process_txt = output_processing(model_result)\n",
    "\n",
    "reference = [ground_truth.split()]\n",
    "model_output = post_process_txt.split()\n",
    "\n",
    "print(f\"dialect: {dialect_txt}\")\n",
    "print(f\"standard: {ground_truth}\")\n",
    "print(f\"translated: {model_result}\")\n",
    "print(f\"post processing: {post_process_txt}\")\n",
    "print(f'BLEU Score: {bleu.sentence_bleu(reference, model_output)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "a729d8b4-60c9-4555-a374-9cd108420c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score as bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "320fc718-0251-47b1-b506-bec25acc56a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 9.291879812217675e-232\n"
     ]
    }
   ],
   "source": [
    "reference = [ground_truth.split()]\n",
    "model_output = post_process_txt.split()\n",
    "# rouge = Rouge()\n",
    "\n",
    "print(f'BLEU Score: {bleu.sentence_bleu(reference, model_output)}')\n",
    "# print(f'Rouge Score: {rouge.get_scores(model_output, reference)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "68322298-1547-44a9-b115-117fa6ec90ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 4498/4498 [16:04<00:00,  4.66it/s]\n"
     ]
    }
   ],
   "source": [
    "bleu_result = {'path': [], 'bleu_score': [], 'dialect': [], 'standard': [], 'predict': []}\n",
    "\n",
    "for sample_path in tqdm(test_txt_path_lst):\n",
    "    with open(sample_path) as f:\n",
    "        sample_json = json.load(f)\n",
    "    \n",
    "    dialect_txt = ' '.join(list(x['dialect'] for x in sample_json['transcription']['segments']))\n",
    "    ground_truth = ' '.join(list(x['dialect'] if x['standard'] is None else x['standard'] for x in sample_json['transcription']['segments']))\n",
    "    model_result = pipe(dialect_txt, num_return_sequences=1, pad_token_id=0)[0]['translation_text']\n",
    "    post_process_txt = output_processing(model_result)\n",
    "\n",
    "    reference = [ground_truth.split()]\n",
    "    model_output = post_process_txt.split()\n",
    "    bleu_score = bleu.sentence_bleu(reference, model_output)\n",
    "\n",
    "    bleu_result['path'].append(os.path.basename(sample_path))\n",
    "    bleu_result['bleu_score'].append(bleu_score)\n",
    "    bleu_result['dialect'].append(dialect_txt)\n",
    "    bleu_result['standard'].append(ground_truth)\n",
    "    bleu_result['predict'].append(post_process_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "6336741d-fff9-4af4-bcb8-250d4d8a4898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3232.568386814992"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bleu_result['bleu_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "197164a4-6fe3-4d5a-a0fd-4ccfd0158a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7186679383759431"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bleu_result['bleu_score']) / len(bleu_result['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "43710baf-e1b2-426d-807b-f42b108d46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_result['predict'] = list(map(lambda x: ' '.join(x), bleu_result['predict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "2f541265-dcb6-4856-869e-9bcbb312dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bleu_result).to_csv('./jeju_bleu_score_result.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "648dc5a5-d43f-44aa-b45e-f29fc7a6301a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': '가서 보니 형은 아무렇지도 않게 마루에 앉았더라    였더라                  '}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(dialect_txt, num_return_sequences=1, pad_token_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "159402b9-2067-42ba-a4b8-92c88c0f55ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['옛날은',\n",
       " '거름에',\n",
       " '보리',\n",
       " '씨를',\n",
       " '섞고',\n",
       " '그',\n",
       " '거름을',\n",
       " '집어',\n",
       " '넣으면서',\n",
       " '보리를',\n",
       " '갈았었어',\n",
       " '',\n",
       " '쌌었어',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_process = pipe(dialect_txt, num_return_sequences=1, pad_token_id=0)[0][\"translation_text\"].split(' ')\n",
    "post_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c67df8b0-b3ef-4a8b-9ebf-12d95c5d553f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'옛날은 거름에 보리 씨를 섞고 그 거름을 집어 넣으면서 보리를 갈았었어'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(post_process[:post_process.index('')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838e644-41a4-4c48-847f-ce8bdd43428e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
